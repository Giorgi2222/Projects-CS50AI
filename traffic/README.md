First I started experimenting with different numbers and sizes of hidden layers and dropout values. I didn't find using multiple hidden layers very useful, I just experimented with the size of hidden layer and dropout, trying to balance on the verge of overfitting. I found multiple combinations that worked well, with the almost same accuracy, I chose one of them. With convolutional layers, I tried to find the optimal number of layers to find the point when the small increase in accuracy isn't worth additional time. I started testing with one layer of Max-pooling with a 2x2 pool size. Not using the pooling layer resulted in a very significant decrease in time efficiency, on the other hand increasing pool size resulted in a decrease in accuracy. I decided to use 2x2 pool size. Between Max-pooling and Average-pooling layers, I chose Average-pooling, which resulted in more accuracy. Lastly, having some kind of understanding in how changes affect the outcome, I experimented with all parameters together and arrived to my final version.
